{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShLV9mW6_wQx"
      },
      "source": [
        "## Step 1: Creating a Knowledge Distillation Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIdI7fuRAWIX"
      },
      "source": [
        "1. The new hyperparameters α and T\n",
        "\n",
        "  - α - control the relative weight of the distillation loss\n",
        "\n",
        "  - T - how much the probability distribution of the labels should be smoothed\n",
        "\n",
        "2. The fine-tuned teacher model, we will use BERT-base.\n",
        "\n",
        "3. A new loss function that combines the cross-entropy loss with the knowledge distillation loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOl95tzXA2-7"
      },
      "source": [
        "Adding the new hyperparameters is quite simple, since we just need to subclass TrainingArguments and include them as new attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ScgwidU7BCmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4b1c43-085e-48d0-ee13-970cc9fad97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Collecting optimum[onnxruntime]\n",
            "  Downloading optimum-1.26.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.53.2)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.33.4)\n",
            "Collecting onnx (from optimum[onnxruntime])\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: datasets>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (4.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (5.29.5)\n",
            "Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime])\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting transformers>=4.29 (from optimum[onnxruntime])\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (1.1.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.11.0->optimum[onnxruntime]) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum[onnxruntime]) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=1.2.1->optimum[onnxruntime]) (2025.7.14)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.11.0->optimum[onnxruntime])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.2.1->optimum[onnxruntime]) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.17.0)\n",
            "Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.26.1-py3-none-any.whl (424 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.6/424.6 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime, transformers, optimum\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.18.0 onnxruntime-1.22.1 optimum-1.26.1 transformers-4.52.4\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers\n",
        "!pip install -U datasets\n",
        "!pip install evaluate\n",
        "!pip install accelerate>=0.20.1\n",
        "# !pip install transformers[torch]\n",
        "!pip install optimum[onnxruntime]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9a4742d2dc4e4000ab8f725f0dddbf9e",
            "8815deb59113429da457f6aabba1d1d2",
            "26b2556cde6f498c853ca41cc1ca90cc",
            "b88a699ef2c245ada51813ca7c570d15",
            "50f81b3b17754f07b5b91d4fc4724593",
            "c84e23e2553a4bb1887df11a0b80d260",
            "c1070c681e87499e98132a7531dbaf07",
            "26ef827ae19b4f5992b24a66d747a31c",
            "57eb5da72d3b46c8a919c5cbfb13be02",
            "36b6399052774eaaa74b68c9929e9165",
            "185eab90562946448a00f4d2f71341b1",
            "44d83794684f4670ae24bb0a43a2a2ec",
            "f0ea8a686a12416982a322ac4d0bbc41",
            "2983dd4f7677489ea48f3bbd0dd6dc80",
            "3092824cea0a4981b17e4d3a74d2e43d",
            "37cb3d95d17d495f8581787a965c82b6",
            "80ca9c5e08c347df8ab5d395556f18cc",
            "c22f2d1bfcd241ab86c46f23204793aa",
            "86980f1dad4c457e9dd5825e506cc17d",
            "7d00d8eb79474e9c86504205ac563a18"
          ]
        },
        "id": "jJ2JOCEbyofx",
        "outputId": "174aeff5-519a-4a72-8714-d8efd7de446b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a4742d2dc4e4000ab8f725f0dddbf9e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XLxPXtH0_xnb"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8YLJHHhA7jT"
      },
      "outputs": [],
      "source": [
        "class KnowledgeDistillationTrainingArguments(TrainingArguments):\n",
        "  def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.alpha = alpha\n",
        "    self.temperature = temperature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jACpKjoRC3yi"
      },
      "source": [
        "# new Loss Function\n",
        "We will subclass Trainer and overriding the compute_loss() method to include the knowledge distillation loss term LKD:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xjHjc3e-B5Xf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eDsHhoN3DHkb"
      },
      "outputs": [],
      "source": [
        "class KnowledgeDistillationTrainer(Trainer):\n",
        "  def __init__(self, *args, teacher_model=None, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.teacher_model = teacher_model\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    #Extract cross-entropy loss and logits from student\n",
        "    outputs_student = model(**inputs)\n",
        "    loss_ce = outputs_student.loss\n",
        "    logits_student = outputs_student.logits\n",
        "\n",
        "    # Extract logits from teacher\n",
        "    outputs_teacher = self.teacher_model(**inputs)\n",
        "    logits_teacher = outputs_teacher.logits\n",
        "\n",
        "     #Computing distillation loss by Softening probabilities\n",
        "    loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    #The reduction=batchmean argument in nn.KLDivLoss() specifies that we average the losses over the batch dimension.\n",
        "    loss_kd = self.args.temperature ** 2 * loss_fct(\n",
        "                F.log_softmax(logits_student / self.args.temperature, dim=-1),\n",
        "                F.softmax(logits_teacher / self.args.temperature, dim=-1))\n",
        "\n",
        "    # Return weighted student loss\n",
        "    loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n",
        "    return (loss, outputs_student) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a60-qPkFWle"
      },
      "source": [
        "## Choosing a Good Student Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8niWzfzOFbSe"
      },
      "source": [
        "How to pick good student model?\n",
        "1. Smaller model than teacher for the student to reduce the latency and memory footprint\n",
        "\n",
        "2. Knowledge distillation functions best when the teacher and learner are of the same model type. (BERT and RoBERTa, can have different output embedding spaces which creates issues for student to mimic the teacher)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv8QmAHFGeQE"
      },
      "source": [
        "In this project, we will use DistilBERT. DistilBERT is a natural candidate to initialize the student with since it has 40% fewer parameters and has been shown to achieve strong results on downstream tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KNDvBb0G7Ie"
      },
      "source": [
        "### load dataset first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XYQfNSWbEMj0"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8r-jEL4Hm4M"
      },
      "source": [
        "We will use CLINC150 dataset which is used to solve the problem of Intent Classification\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "08NzJ3g6HksW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "4ba12202cb1548939dfe36f3e92d2e22",
            "45919e202de7439bb75485b0b29837c8",
            "afb65b2c89bb48b19368d308babf320e",
            "335d90f35bbc46f0b7303838f9bd1019",
            "0bbf1c6e441a4bd7a1e728623f977fd4",
            "4a4ccd1d7a2d40d2af2125b8c8a769f9",
            "8a1cde3928ef41ec99fc704ab35032ed",
            "fa249bc0f11d42b4b9a8918dd18db5cc",
            "4f6b8c3575fb4e44a819716407f63efa",
            "f17a2e641a904fe7a5ac0beb85a6ea61",
            "aa3385f650ac464a8c259c079e231850",
            "fd14d826d20f4c84b2e57facb5abfeac",
            "c42a7d8d28264017a927ab7bf92c2dae",
            "5b6c7b19b50a45afbaa16db1d5f7db72",
            "04a4f33ff48042568138a71f13c56542",
            "74182d27bd644611ac57f74a0cdee8dc",
            "efcd7532f07e471a80c22607dac56873",
            "a668d9b36b63419bbae2dd07ca086d2e",
            "a2b537a89a4e4b53a4b66e97016acf87",
            "c67ed01c6cd94cc2b8276a5fe17b900b",
            "2e5f2b61fed642f7937ebae5ce4a3278",
            "c59ba7ba6ff64879858578d45ffe040c",
            "d33804b45bc54e7e869c75ba206c71e7",
            "dc8b7cfdc0f344ed9efc9e378c7a6254",
            "950f2e70f7d941058d4ff0efcef47878",
            "bed2caf4e1af4cbd9268d9e83a3751b4",
            "90a879ad57b344f8a536b9e36a96aad8",
            "07c38e38c3e44c0c84bb5ceb6d4f780b",
            "fca506e5b7d34fb5b37e99fe1bca1aa4",
            "bf09046ba52c4c4997273552756d8bc0",
            "93f511071c364d0db18e3458e71cd4db",
            "4d2d2b1c77de42e0ba88dfe923245230",
            "42ecf707803b4068abe45de1f695fbaf",
            "0c04310c720e49f2b4f31154e3cd6cda",
            "3f913d152eb04ebc9e56025faef9cb0e",
            "a8dbed1be1664ffdbea019a9674871c9",
            "fd6f3cf622cc41bdb8243c505b735e9b",
            "ca56ab633035409d85a42becbd450644",
            "8d528933f6c345d7ae7590dfd1627a20",
            "39471d579ea84b788fc53c0041577314",
            "635f77b180e548ecab8bb5f3b110e813",
            "c170dd0a5dbc4c8dbab5734526424786",
            "d37055755d1f40f9a7842db334dfc068",
            "5c994c6fc9ab485487b8d665e95df3b6",
            "26a4aa1c53a64acf97c27c13e5687ec8",
            "a29375148dca4b7cab13519933c70a1c",
            "75d2dbcb8f3d42a1a217d9fd6bc23e68",
            "ed2ba5578f2c4b568964639f251f0dff",
            "2491324aae394d69b6e5528c0c63f19d",
            "1b92752940f640418cc33f1d1e04270b",
            "49a773aa980d42c5a092ca57cf6f03e2",
            "97f7be405a974fd19fd02dcaede6c4b2",
            "04291dcf53b84794a083b6849f5a2b60",
            "658445932c9c46c48ed26c1e2908a677",
            "53497b18679c4c63a4a96c444062d4d0",
            "9d7ef9abf3a349a78f9cf7b5bd4f4a6f",
            "8ccf7f4fb0944783be5c4b220ad21da7",
            "72b11733ccc84dabba3127af389a7bfd",
            "2cab0eaf5a144991aa8946a5960d8337",
            "1a54f47547d24713abdaa82c36e2d1c5",
            "acb9f7e243054f869b5e540426f05a9d",
            "cd7fee4d28e84b5bbdd94a2a7216318b",
            "ff0dbdd764a0499b864622ecff8a0501",
            "b8ced939217b47fb9d389804c3df7c56",
            "b94eb9326b664926aebbe05928a3c650",
            "5402344acf644ef8af40fb6b7865e697",
            "62302f54299d43df9feb79a34c625920",
            "ec24d36e818b423fa8518e81e6a4cd44",
            "a491ca8aa9cc4c9a89af76c3f8479082",
            "f3c7d77aa8ed45b7952d5a0c0a5bc765",
            "3b77994044e14fd290825b6a83b30588",
            "dec76f15056945c5b8b6feb1eb4ecb14",
            "3c22cf788de345fa87645ac6d1a8d843",
            "c26df3a1a8a34b9080a318d629da03f4",
            "d0095c2955db4d24a768c9ade6547213",
            "591fbfa7f9e8447baa0c8cbbac2d5851",
            "bf56acc284f94b88b72ecfbdc766e936"
          ]
        },
        "outputId": "b9a56d8b-e363-48ce-b050-22fe178fbcce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ba12202cb1548939dfe36f3e92d2e22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/312k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd14d826d20f4c84b2e57facb5abfeac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33804b45bc54e7e869c75ba206c71e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/136k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c04310c720e49f2b4f31154e3cd6cda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/15250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26a4aa1c53a64acf97c27c13e5687ec8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d7ef9abf3a349a78f9cf7b5bd4f4a6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/5500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62302f54299d43df9feb79a34c625920"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
        "#the plus configuration refers to the subset that contains the out-of-scope training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sxloSRf8HvZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c68183-9b71-4050-9d73-cb43d3aa34ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'what expression would i use to say i love you if i were an italian', 'intent': 61}\n"
          ]
        }
      ],
      "source": [
        "sample = clinc[\"train\"][0]\n",
        "print(sample)\n",
        "#Each example in the CLINC150 dataset consists of a query in the text column and its corresponding intent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPq0YTLrH7oj"
      },
      "source": [
        "The intents are provided as IDs, but we can easily get the mapping to strings (and vice versa) by accessing the features attribute of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KJEaBRWpHzWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfc0998-2d1d-468a-fe03-2ebe289d9bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translate\n"
          ]
        }
      ],
      "source": [
        "intents = clinc[\"train\"].features[\"intent\"]\n",
        "intent = intents.int2str(sample[\"intent\"])\n",
        "print(intent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_pHhDuMILnc"
      },
      "source": [
        "#Lets preprocess or tokenize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0kxUuVeHH5r0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u7kEoYAoIPwF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "36e8f13d36464954801a8741e2304ab6",
            "b05951fbceb149c78f15d77fa857e9ea",
            "8fad15d2ac1a4795a5cd5060181464aa",
            "f038f8433c1d4bfcafbdb3afd74d13bb",
            "615110fe976a4a658f270e82d6a1ec79",
            "93f0ddf6398a4131a82e1d475eb3fe48",
            "e57bf15a170f4dbbb5adef16893abcbd",
            "8e15f8a0b7ca4cedbd17733c2f609948",
            "41602295cd6c43b19fe50334930913fa",
            "37307b6800004a799f004812cc07a231",
            "e89a81f643414582bee4f87aafc31fda",
            "4f7ec3bc9cdd4acfbea803b91f8652d7",
            "c0eead06d4df4af3b3e555dcaf47834f",
            "fa4636693e394a5e831b52a77cd50063",
            "11d567243f05470cb214ece856962969",
            "872b3e020e464bebb0b23736cbaa47d1",
            "d2adf0ba3df24839abd7551ec873f184",
            "248753ba02504a7eb20c661022e950e0",
            "28d6d79a5e17487f9c2b0a70dc9eb2c8",
            "ddd5aa2becc84d07b8cab5ad5437af4a",
            "aaca58fec83e45b79e05bb861451cc19",
            "8c62a34ce6d14e2783664008ac567d27",
            "5fe31fd8e58b4a1ca91410a03daaa4d0",
            "be55a3af37ea417185dd87f767e8db81",
            "182a1bddfdf24935aa6093ed8d3c742f",
            "df9bee23a5ac4aaf88e934398c72d504",
            "506259de6a9a41d5a12442692bad5ae5",
            "2ec3bc5de83a43bc9d13767940502c65",
            "3b294be7f39748639237c921c6162cdc",
            "d9b418bddca647e8b2e0e41eb09bd809",
            "a0196078615a494d89b436098b198d88",
            "6dce93006c6f42a3aecc82739c00b997",
            "d3f5a04d697e4117b11b73769d96db3c",
            "88d1869ac1984c558e9cb32072a335e6",
            "32113777a0ba4842a92b24f41cf5afb2",
            "e48ae58abf1242fbb83dae4717fe837c",
            "384d5987ffea4d86abb6452c5ba6047e",
            "f78c3865470d4a0fa8d695d91f01190d",
            "d8a6c9dccc934bc1b0db0aadb4f981b0",
            "73a72aa7a5d749b5aa26f508f1c93fb1",
            "dd265d9250f044e1932730de19f8826a",
            "600f849b787f4736b90197373b74aff1",
            "167ad3cf43134b1283d2f089877102aa",
            "abd0358c0dcb4c0e95e7461156ece988"
          ]
        },
        "outputId": "295717c0-b9ee-4016-f419-461051fbe80c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36e8f13d36464954801a8741e2304ab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f7ec3bc9cdd4acfbea803b91f8652d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fe31fd8e58b4a1ca91410a03daaa4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88d1869ac1984c558e9cb32072a335e6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "student_checkpoint = \"distilbert-base-uncased\"\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(student_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e71qdL6xIT3m"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(batch):\n",
        "  return student_tokenizer(batch[\"text\"], truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zP064S4sIa6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e3ef50fb66cf4426ad7cde6459990910",
            "ea0dccd454814e668d9bdff2d8c2f222",
            "82994f1f9c5740f480de4d18610e3b6d",
            "0869a67b6fa5470c83fc32cc26d62ff0",
            "96820ba87e3548ab86ba7e6c080040cd",
            "a2500ea9098c44e6a8e0a5f59b3082c7",
            "b348c8c9f0614750b291de7682b18dd4",
            "7151ae7ca960404ea964590f172ec408",
            "17ae60022018443487517e044f37eb95",
            "a3df33b3bbe241918b6274cc9f076786",
            "5eb9898fa65f43c9878d0752689dc649",
            "81e6928f32c349e5ae46db4084a95a8b",
            "6b2dce7cf6fb4246844f1c6d0412d6df",
            "a5a382aa1a18464fa6f2df42f95e7fdd",
            "cd774b402e6647cca28a1fb4d5b34af8",
            "c5bd97048cd84e669bd18741daef7a7a",
            "76580ae35d314417b537a1beed5db57c",
            "60afbef38b0a490faf42b5bef60a8b2d",
            "e9ac90eb019947b6b0b9d4c8b31dd0e7",
            "3308bc02c331423887839184f5ee75b7",
            "9db0272b6ad8429faa70380f09b84b18",
            "f0523867b9f04104a32bfed02778a79d",
            "5b8b49c2679845b8bdcac9b94d2a1642",
            "e7a6a64e44e143cfb0318e747a27d309",
            "d10dd29fa0264bfcab4431b7ec99436a",
            "2055d1d1e5d544f885e89c764e087d69",
            "9a9193ce556c42ea96154808b3c95c86",
            "ace6f80d485c4fd9846fb716d54a7816",
            "cb265d263b184cc292cf73142e05c980",
            "ea653ae6b8ec48cd8adc873881aff539",
            "45287f5bcfdf442c9ee120461dcd230e",
            "23544c9051534732923f40dff0e7050b",
            "3e7ef79fab5a4c9bbabff7598b6c2e02"
          ]
        },
        "outputId": "de15ef53-e6fe-44d8-e56c-833913299539"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3ef50fb66cf4426ad7cde6459990910"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81e6928f32c349e5ae46db4084a95a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b8b49c2679845b8bdcac9b94d2a1642"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "clinc_tokenized = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "#We will remove text column as we don't need it\n",
        "#We will also rename the intent column to labels so it can be automatically detected by the trainer.\n",
        "clinc_tokenized = clinc_tokenized.rename_column(\"intent\", \"labels\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fSUsRbgKHq7"
      },
      "source": [
        "# Define metrics for DistillationTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HpUI6EZiKK-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "12e006121b3e468e8d3fa77962a8646e",
            "97f78cd7c22741ada27dc80aa8a5fe6d",
            "b8d6f4bc090245b1a02bbb129f62ed73",
            "0c3eb3ccf53540d5a2a64c89f1a5ad45",
            "0c121bb493f54846bf9f323958bdebca",
            "4d5c60f9b3dd47ba95be28b47b18d9c5",
            "8f2227b5220c4b7c90a6d86103381fae",
            "9849d9b07e9640cfbbf26f62cba334bf",
            "b082086aec344b6c9ca65402c066fcba",
            "51e2e418cb32422e91b29cf4811b8ec9",
            "156351d97edd470792c31c0f84c4561c"
          ]
        },
        "outputId": "991e9f1d-23d7-436c-a1b5-1d3775e4cebe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12e006121b3e468e8d3fa77962a8646e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "accuracy_score = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  predictions, labels = pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  return accuracy_score.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNXBLatvKcmv"
      },
      "source": [
        "In this function, the predictions from the sequence modeling head come in the form of logits, so we use the np.argmax() function to find the most confident class prediction and compare that against the ground truth label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi074k0aKgdm"
      },
      "source": [
        "# Training Arguments for DistillationTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Jh-LFz3QKS0Q"
      },
      "outputs": [],
      "source": [
        "batch_size = 48\n",
        "finetuned_student_ckpt = \"distilbert-base-uncased-finetuned-clinc-student\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "H7epVe7uKmBb"
      },
      "outputs": [],
      "source": [
        "hub_model_id = f\"Vyshnev/{finetuned_student_ckpt}\"\n",
        "\n",
        "student_training_args = KnowledgeDistillationTrainingArguments(\n",
        "    output_dir=finetuned_student_ckpt, eval_strategy = \"epoch\",\n",
        "    num_train_epochs=5, learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    alpha=0.5,\n",
        "    weight_decay=0.01,\n",
        "    run_name=\"distilled_model\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=hub_model_id,\n",
        "    hub_strategy=\"every_save\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-DTzbcvNLdX"
      },
      "source": [
        "## Lets initialize student model but before that provide the student model with the mappings between each intent and label ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CXrAiUxnKv1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17364b95-d798-417a-939d-1e0792c1180b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
        "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
        "\n",
        "id2label = pipe.model.config.id2label\n",
        "label2id = pipe.model.config.label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6Q5mVIMzNTry"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "num_labels = intents.num_classes\n",
        "student_config = (AutoConfig\n",
        "                  .from_pretrained(student_checkpoint, num_labels=num_labels,\n",
        "                                    id2label=id2label, label2id=label2id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "K5ntdpKzPF20"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def student_init():\n",
        "  return (AutoModelForSequenceClassification.from_pretrained(student_checkpoint, config=student_config).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_pwSM8aPb4r"
      },
      "source": [
        "## Load teacher checkpoint and start finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xfZFhd-_PVCX"
      },
      "outputs": [],
      "source": [
        "teacher_checkpoint = \"transformersbook/bert-base-uncased-finetuned-clinc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "RljObrzrPlKx"
      },
      "outputs": [],
      "source": [
        "teacher_model = (AutoModelForSequenceClassification\n",
        "                     .from_pretrained(teacher_checkpoint, num_labels=num_labels)\n",
        "                     .to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "N-vXAa1iPqJ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe1bcfa-b69e-474b-f4a2-4f137baf859e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2594125412.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KnowledgeDistillationTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# start the training\n",
        "distilbert_trainer = KnowledgeDistillationTrainer(model_init=student_init,\n",
        "        teacher_model=teacher_model, args=student_training_args,\n",
        "        train_dataset=clinc_tokenized['train'], eval_dataset=clinc_tokenized['validation'],\n",
        "        compute_metrics=compute_metrics, tokenizer=student_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "TFxG3hRDLQd1",
        "outputId": "71fe7ced-718c-42b2-9867-0e9c25c3d990"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1590' max='1590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1590/1590 09:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.929531</td>\n",
              "      <td>0.710645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.277400</td>\n",
              "      <td>1.116900</td>\n",
              "      <td>0.843871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.277400</td>\n",
              "      <td>0.738993</td>\n",
              "      <td>0.896774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.045700</td>\n",
              "      <td>0.589131</td>\n",
              "      <td>0.916129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.627700</td>\n",
              "      <td>0.546609</td>\n",
              "      <td>0.924839</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1590, training_loss=1.2736781258253183, metrics={'train_runtime': 567.0132, 'train_samples_per_second': 134.477, 'train_steps_per_second': 2.804, 'total_flos': 414689637990180.0, 'train_loss': 1.2736781258253183, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the Distilled Student Model to HF\n",
        "distilbert_trainer.push_to_hub(commit_message=\"End of training\")\n",
        "\n",
        "print(f\"Model pushed to: https://huggingface.co/{hub_model_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7cafda17ef1640a1901c2f3a80f90b3f",
            "9fb67bd1048f4393bff9fe584f5f5fd5",
            "b0066ef5f95e4aff824926e48b0daf2b",
            "3b4fc78c7cfe48cbbb1a9095c84f2eec",
            "3bc6c26430394c6fb4f907ebbc4a195e",
            "606581b52c6e4e3483a2df2593df5f86",
            "847055620d1f4bb2b4921265e2e3adb0",
            "6e4baf56f2b44c39952eed8d6ba5bbf6",
            "7c1e3f94a9e341808c68a249aa7613fa",
            "cd76b1059c8847f980d08ef25187c9df",
            "0b46210014654a449e29a5ac64e91ed0"
          ]
        },
        "id": "zqAkYUCx2bTM",
        "outputId": "583542e2-1a89-4f81-cf9f-26e57be26d1f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "events.out.tfevents.1752749487.57e547ed9e48.339.1:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cafda17ef1640a1901c2f3a80f90b3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model pushed to: https://huggingface.co/Vyshnev/distilbert-base-uncased-finetuned-clinc-student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Teacher and Student Model"
      ],
      "metadata": {
        "id": "fKOilJSN-M3d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "y2BX_vmsQU2a"
      },
      "outputs": [],
      "source": [
        "#We will compare the two models based on size and inference time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Teacher and Student model and then computing model's size in MB"
      ],
      "metadata": {
        "id": "gbgDGiD7IWHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_teacher_model():\n",
        "  teacher_model.save_pretrained(\"teacher_model\")\n",
        "def save_student_model():\n",
        "  distilbert_trainer.save_model('student_model')"
      ],
      "metadata": {
        "id": "tY0C3ZsdIVaK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_teacher_model()\n",
        "save_student_model()"
      ],
      "metadata": {
        "id": "DU0EuD7UI9ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ad7532-9256-477e-c69e-6a55e52b5436"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "import os\n",
        "\n",
        "def compute_parameters(model_path):\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "  parameters = model.num_parameters()\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "L7LJRPRzJkOg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_model_parameters = compute_parameters(model_path=\"/content/teacher_model\")\n",
        "print(\"Teacher Model: \", teacher_model_parameters)"
      ],
      "metadata": {
        "id": "8-leprTuIE2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f52646-f8a0-42ac-add1-1c5a744484ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model:  109598359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "student_model_parameters = compute_parameters(model_path=\"/content/student_model\")\n",
        "print(\"Student Model: \", student_model_parameters)"
      ],
      "metadata": {
        "id": "Plfdn76CLKVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1179c6-9216-4d46-b706-d538298a89ba"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model:  67069591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decrease = (student_model_parameters-teacher_model_parameters)/teacher_model_parameters\n",
        "print(decrease*100)"
      ],
      "metadata": {
        "id": "EHRy48NBNPV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea66e433-e9fc-494c-e107-b383a857b03c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-38.804201438818986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/student_model -al --block-size=MB"
      ],
      "metadata": {
        "id": "zQsbyCP7Lfgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30de24cb-b9e6-4773-8556-c383346b1a9b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 270MB\n",
            "drwxr-xr-x 2 root root   1MB Jul 17 11:06 .\n",
            "drwxr-xr-x 1 root root   1MB Jul 17 11:06 ..\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 config.json\n",
            "-rw-r--r-- 1 root root 269MB Jul 17 11:06 model.safetensors\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 tokenizer.json\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 training_args.bin\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:06 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/teacher_model -al --block-size=MB"
      ],
      "metadata": {
        "id": "vPduu_l7MZeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39397e2f-2a3c-46d3-a6d7-4f2d7d1fd3b4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 439MB\n",
            "drwxr-xr-x 2 root root   1MB Jul 17 11:05 .\n",
            "drwxr-xr-x 1 root root   1MB Jul 17 11:06 ..\n",
            "-rw-r--r-- 1 root root   1MB Jul 17 11:05 config.json\n",
            "-rw-r--r-- 1 root root 439MB Jul 17 11:06 model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clinc['train']['text'][101])\n",
        "print(clinc['train']['intent'][101])"
      ],
      "metadata": {
        "id": "Og3MC2FOMh_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a20a086-1c00-4718-e592-bfa851dd79ca"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complete a transaction from savings to checking of $20000\n",
            "133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we will take average times of multiple inferences on same input"
      ],
      "metadata": {
        "id": "Hhz7qrUoNzJ0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets warmup first\n",
        "from transformers import pipeline\n",
        "import time\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"/content/teacher_model\", tokenizer='bert-base-uncased')\n",
        "\n",
        "sample_input = clinc['train']['text'][101]\n",
        "\n",
        "#WARMUP\n",
        "for _ in range(10):\n",
        "  _ = pipe(sample_input)\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "  _ = pipe(sample_input)\n",
        "total_time_teacher_model = time.time()-start\n",
        "print(\"Total time to process 100 requests for Teacher Model: \",total_time_teacher_model)"
      ],
      "metadata": {
        "id": "22AhklIhOEFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "0e48f3ea64b84c1b81d59e9dbbb40e81",
            "8defbfefb4954bfcbd47e25866e4db73",
            "948dfc4f670a4557863051ea95ddac07",
            "6d7acf60e7ab44e0a7d00b60649e9595",
            "a96c1a73010748269bcbcea8e63655a7",
            "86133e5020d34c298f821603d5e175ec",
            "88b94e17dbaf4fa4afd4b284a249bc76",
            "fd09123ac98f457dae0322dfbe0eab42",
            "d7d8837e11794211a33dd875d0b386b9",
            "46c632495c8c4e0692e9ebca84cbd2b0",
            "cfb56bad568246a8b4e7c02a2eb3620f",
            "5e06465c122b47ca8890ec352169d538",
            "65565eb68dd440dfb1845f13a7dc1ac8",
            "0b5bd233e93f4ac9894631362f6a7e6f",
            "a17ff7c974f14056b6d7d125633234a2",
            "a9cadd63cce54b6a8965fabf181a6847",
            "4d297698564a4a269ab749d3be0e301d",
            "39a5e95f79c046cdae7c85a2afb426e6",
            "c0dacb415ae24849b6ad37743222ebc0",
            "064fea1bf23645ff9469337e1b4b8469",
            "63ade60440be4676970372527cbc5cd1",
            "92d8ee0590394f378fe0c4f261655e15",
            "165c5ef4f34d4c46a8c8172ef58a1b55",
            "b9982f41ca164d92aa6d9a30710679bc",
            "a998c9c36e784869af4d829fe9bea179",
            "f3eb59112d064bee92da9c67fc8bd8f3",
            "07a5cf6c3ca943ba864bf2547a3af636",
            "47da6b26f3ea49a9b939a11ed53fb14f",
            "f25d0aacdf144d84898313994077866e",
            "aac70e59c2ec4806b8530be89464c35c",
            "017732b0dfda4f3490e365362b3b3738",
            "2bb9a3f4692d43de8a662f4c4476bc14",
            "b0fcf00a4ca8444983cf5e19a3317ad0",
            "121f921fbd6a434e99d7d828aa32a23a",
            "d0e71780b64a4989b08ef9e0541c3662",
            "f6f691bae7644acb8c2f71b6a49095a8",
            "a47642461cd34d8698bbc908af57280e",
            "d7cbdb82719d44238a3d36c991955168",
            "b7e398f2e84e4de5a8c1dd2c37ba11d8",
            "7fe60c10605e48c0ae2332928795f17f",
            "d48f4e5e4ea747139d47e6c63a700d52",
            "c44f29ac7c5946a9aa13ddf029a826ad",
            "eef5e5e059f147948e20220cc5bdde1c",
            "024571dcb5854db1b0ac7648c035c7c9"
          ]
        },
        "outputId": "e293d84b-57c8-4a34-e8f4-0b617396225a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e48f3ea64b84c1b81d59e9dbbb40e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e06465c122b47ca8890ec352169d538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "165c5ef4f34d4c46a8c8172ef58a1b55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "121f921fbd6a434e99d7d828aa32a23a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time to process 100 requests for Teacher Model:  0.9262676239013672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-classification\", model=\"/content/student_model\", tokenizer=\"distilbert-base-uncased\")\n",
        "\n",
        "sample_input = clinc['train']['text'][101]\n",
        "\n",
        "#WARMUP\n",
        "for _ in range(10):\n",
        "  _ = pipe(sample_input)\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "  _ = pipe(sample_input)\n",
        "total_time_student_model = time.time()-start\n",
        "\n",
        "print(\"Total time to process 100 requests for Student Model: \",total_time_student_model)"
      ],
      "metadata": {
        "id": "ND0Rk_c-Od-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e201cd-c06a-4d4a-ab0a-7efa4dbda1bf"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time to process 100 requests for Student Model:  0.5403509140014648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decrease_in_time = (total_time_teacher_model-total_time_student_model)/total_time_teacher_model\n",
        "print(decrease_in_time*100)"
      ],
      "metadata": {
        "id": "5XkDVWpQnHK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8dd7ac-6d29-4d90-c836-5d2ec6f5e06f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41.66362938115565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Post-Training Quantization of the Student Model\n",
        "\n",
        "Now that we have a smaller, distilled student model, we can optimize it further using quantization. This process reduces the precision of the model's weights (e.g., from 32-bit floats to 8-bit integers)."
      ],
      "metadata": {
        "id": "RSDHGgXkBxwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n",
        "# Import QuantizationConfig and QuantizationMode\n",
        "from optimum.onnxruntime import QuantizationConfig, QuantizationMode\n",
        "\n",
        "# --- Define all the paths we'll need ---\n",
        "# Path to the fine-tuned PyTorch student model on the Hub\n",
        "pytorch_model_path = \"Vyshnev/distilbert-base-uncased-finetuned-clinc-student\"\n",
        "# Path where the intermediate, unquantized ONNX model will be saved\n",
        "onnx_model_path = \"/content/student_model_onnx\"\n",
        "# Path where the final, quantized ONNX model will be saved\n",
        "quantized_model_path = \"/content/student_model_quantized_onnx\"\n",
        "\n",
        "# --- Step 1: Export the fine-tuned PyTorch model to ONNX format ---\n",
        "print(\"Exporting PyTorch model to ONNX format...\")\n",
        "# We use ORTModelForSequenceClassification to handle the export\n",
        "onnx_model = ORTModelForSequenceClassification.from_pretrained(pytorch_model_path, export=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pytorch_model_path)\n",
        "\n",
        "# Save the exported ONNX model and tokenizer to a new directory\n",
        "onnx_model.save_pretrained(onnx_model_path)\n",
        "tokenizer.save_pretrained(onnx_model_path)\n",
        "print(f\"ONNX model saved to: {onnx_model_path}\")\n",
        "\n",
        "\n",
        "# --- Step 2: Quantize the exported ONNX model ---\n",
        "print(\"\\nQuantizing the ONNX model...\")\n",
        "# Create the quantizer from the ONNX model directory\n",
        "quantizer = ORTQuantizer.from_pretrained(onnx_model_path)\n",
        "\n",
        "# --- THE FINAL STEP---\n",
        "# Define the quantization configuration for dynamic quantization.\n",
        "qconfig = QuantizationConfig(\n",
        "    is_static=False,\n",
        "    mode=QuantizationMode.IntegerOps,  # This is the key argument for dynamic quantization\n",
        "    per_channel=False,\n",
        "    format=None\n",
        ")\n",
        "\n",
        "# Apply quantization and save the final model\n",
        "quantizer.quantize(save_dir=quantized_model_path, quantization_config=qconfig)\n",
        "\n",
        "print(f\"Quantized ONNX model saved to: {quantized_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296,
          "referenced_widgets": [
            "bd0241ed56a34372844c81402eb01630",
            "f27e0d92f9a741348d4a4663e75cea98",
            "425ee1a87f5e4fae83ce72f6d3735bf1",
            "5f91580888194ba791a2828c4c479ada",
            "a00d32f5018a4c7bb05773b69432b399",
            "c0d86acc79294a649461c021c4b238b4",
            "b6f184162f2c40ee8260f7406fe651c6",
            "802cf8a2b84948898ee32132c6551d4a",
            "ae6a7633db6b44829989bb514c1c4625",
            "d1693aa624e24269b5203b76892e5f44",
            "5d23e7a4c42a41379a42bfcca0d8cc80",
            "dba71d7ef2eb4ae8a9e02f83389a2fde",
            "f0c6b649650a4489bbde8832f271cf91",
            "021696f149ac421fb5f3088f8ad1e5c6",
            "95cd8a2461d643a797ac78f1f2e738d9",
            "4ca76edf579445bc9b4510ec5a232fcd",
            "b3308888b4dd4954a67d116f6f70e948",
            "4964ff428b344c7f826657a3a217ea3d",
            "1475e223ab2d441289218aa0f96d2295",
            "48bccc714b0844988b94f93725f214c8",
            "5e77a86d420c4924ac61eaff7d12a626",
            "3511a9303ff342ffa0c62bf295a20408",
            "545ea50a86534bcd983f94b47f06d9e6",
            "2662eadb80aa4452a48e6cd35df0b9df",
            "e7feedde186149f5bd3f01c86551f314",
            "95abdcc1191f451298f4080865bec240",
            "7de2ede8c41940aaacd1c4343f1f8e93",
            "a808209523fb4340a8edda493cbc30a0",
            "fdb927b07a184849b2350cc95b4c0677",
            "623c31ea0a8e4b3e998a67838603ccd6",
            "925fe2ea80724ee6ba8904a9290cf1e7",
            "0ddfd86ff67f436d8b32153845e2fdb0",
            "632d10fc0c204b7d988f5c772f198b5f",
            "ff7a2298722f46aba1b6bf719b193aca",
            "8bb106fb05584b7996cc9ed6f82b45ed",
            "29ffdb19fcac4da1b898ad1e9974128d",
            "8ea5d4c9470740719a7eaf90fa474410",
            "32def3c654054ecc84b98216ca78f33d",
            "eec9c5e074ea425bb9e11008443faa48",
            "34e315a2b6a94e9483d3bc81ce953bb9",
            "415356c7ce0f45468f24590331094a14",
            "70dc43566f94475ea23b00be2377face",
            "2df38099b74e4760af44b2e6e83f493e",
            "aa4fbe15fb5b4475b08b583093909b19",
            "bac9ac9d75f5409dba0015cbf09d3f3e",
            "5538e4609f414d04b9910c45dd6b9ec8",
            "eeb7f4f045ef48bb988912126d284693",
            "4478fac802064671ac08690e12080f81",
            "0e785004fdd1464a906b4e2d1d1a6944",
            "2ae860391ab1431ea7db86452a0d6dd3",
            "c563c2ca668b43fdb1d4d1e7099f870f",
            "f6e0435e69164023b0928286b061f3b9",
            "3c04996fb51d4bf092134f97d25a562c",
            "d96c0a9691cf4af0bd6b42837fb86ac2",
            "97f848ad8dfb4691a9284804c06f81a8",
            "26f14b4b4ec544d4aaced352e319ae0a",
            "4c250f6012444e12866ea0e287fc343b",
            "0d426715704b4fcf84c80e3c1249c389",
            "88e52fbd1b854c78b123585809937946",
            "f8326b66299c4c9595ce7574b447657f",
            "1c5152953c5740638ceccb21885f878f",
            "45d8c489e892437caf1ce392a1f82fb6",
            "566c14adf4764650b2bde15e1ed608ef",
            "d28edff2e3974d7eb6f7933f5b2dfcd9",
            "a751f63c8a634608a165be6b5ebea70a",
            "b408c627accb4d2d8966c072a48bf0a2"
          ]
        },
        "id": "H7kJU689B49U",
        "outputId": "05637e86-803a-4ca8-8e12-76f748e4bf1c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting PyTorch model to ONNX format...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd0241ed56a34372844c81402eb01630"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dba71d7ef2eb4ae8a9e02f83389a2fde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "545ea50a86534bcd983f94b47f06d9e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff7a2298722f46aba1b6bf719b193aca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bac9ac9d75f5409dba0015cbf09d3f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26f14b4b4ec544d4aaced352e319ae0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model saved to: /content/student_model_onnx\n",
            "\n",
            "Quantizing the ONNX model...\n",
            "Quantized ONNX model saved to: /content/student_model_quantized_onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the quantization is completed and its saved in /content/student_model_quantized_onnx directory\n",
        "\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the local quantized model files\n",
        "quantized_model = ORTModelForSequenceClassification.from_pretrained(quantized_model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(quantized_model_path)\n",
        "\n",
        "# Define a new Hub ID for the quantized model\n",
        "quantized_student_hub_id = \"Vyshnev/distilbert-base-uncased-clinc-student-quantized-onnx\"\n",
        "\n",
        "# Push the quantized model and tokenizer\n",
        "print(\"Pushing quantized ONNX model to the Hub...\")\n",
        "quantized_model.push_to_hub(\n",
        "    save_directory=quantized_model_path,\n",
        "    repository_id=quantized_student_hub_id\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    repo_id=quantized_student_hub_id,\n",
        "    commit_message=\"Added tokenizer\"\n",
        ")\n",
        "\n",
        "print(f\"Successfully pushed quantized model to: https://huggingface.co/{quantized_student_hub_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "12dce1cb01a14d76b358f30dd2eb484e",
            "8babf5ee7fcb45b4a74fd049d9923b74",
            "9b9db6a10cce4b5c939e9204e01f278d",
            "0f6a7188fb444beaa27d1ee6607a8f54",
            "ae9b2d07259c41d8a7f0c1e5f3b77c1a",
            "801b23945f0b4c83af35305d21842e4a",
            "d626f114e75f4561a8ab8c8660ffae13",
            "25b2416f8e6642378af5073d0ea30436",
            "a8639e03b93a45b7971b275d054d76f2",
            "c949f174a87146fea8a74ca7ddcd8c4f",
            "41c3e69e09f14481a3ef5406dfd03e36"
          ]
        },
        "id": "pf1MaPQp5qYw",
        "outputId": "12a28f8e-52d5-4259-fd41-4a290fab2cda"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pushing quantized ONNX model to the Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model_quantized.onnx:   0%|          | 0.00/67.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12dce1cb01a14d76b358f30dd2eb484e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully pushed quantized model to: https://huggingface.co/Vyshnev/distilbert-base-uncased-clinc-student-quantized-onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# evaluate the results\n",
        "\n",
        "# --- 1. Compare Model Size ---\n",
        "\n",
        "local_student_model_path = \"distilbert-base-uncased-finetuned-clinc-student\"\n",
        "\n",
        "# Original student model size\n",
        "original_size_bytes = os.path.getsize(os.path.join(local_student_model_path, \"model.safetensors\"))\n",
        "original_size_mb = original_size_bytes / (1024 * 1024)\n",
        "\n",
        "# Quantized ONNX model size\n",
        "quantized_size_bytes = os.path.getsize(os.path.join(quantized_model_path, \"model_quantized.onnx\"))\n",
        "quantized_size_mb = quantized_size_bytes / (1024 * 1024)\n",
        "\n",
        "print(\"--- Model Size Comparison ---\")\n",
        "print(f\"Original Distilled Student Model Size: {original_size_mb:.2f} MB\")\n",
        "print(f\"Quantized ONNX Student Model Size: {quantized_size_mb:.2f} MB\")\n",
        "size_reduction = ((original_size_mb - quantized_size_mb) / original_size_mb) * 100\n",
        "print(f\"Size reduction of: {size_reduction:.2f}%\\n\")\n",
        "\n",
        "\n",
        "# --- 2. Compare Inference Latency ---\n",
        "print(\"--- Inference Latency Comparison ---\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the original (unquantized) student model pipeline\n",
        "original_pipe = pipeline(\"text-classification\", model=pytorch_model_path, tokenizer=pytorch_model_path, device=-1) # Use CPU for a fair comparison\n",
        "\n",
        "# Load the quantized model pipeline\n",
        "# For ONNX models, we must explicitly provide the tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(pytorch_model_path)\n",
        "quantized_model = ORTModelForSequenceClassification.from_pretrained(quantized_model_path)\n",
        "quantized_pipe = pipeline(\"text-classification\", model=quantized_model, tokenizer=tokenizer, device=-1) # Use CPU\n",
        "\n",
        "# Use the same sample input from before\n",
        "sample_input = clinc['train']['text'][101]\n",
        "\n",
        "# Function to benchmark latency\n",
        "def benchmark_latency(pipe, text, num_runs=100):\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = pipe(text)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        _ = pipe(text)\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = end_time - start_time\n",
        "    avg_latency_ms = (total_time / num_runs) * 1000\n",
        "    return avg_latency_ms\n",
        "\n",
        "# Run benchmarks\n",
        "original_latency = benchmark_latency(original_pipe, sample_input)\n",
        "print(f\"Original Student Model - Average Latency: {original_latency:.2f} ms/request\")\n",
        "\n",
        "quantized_latency = benchmark_latency(quantized_pipe, sample_input)\n",
        "print(f\"Quantized Student Model - Average Latency: {quantized_latency:.2f} ms/request\")\n",
        "\n",
        "speedup = original_latency / quantized_latency\n",
        "print(f\"Achieved a speedup of {speedup:.2f}x\\n\")\n",
        "\n",
        "# --- 3. Compare Accuracy ---\n",
        "print(\"--- Accuracy Comparison ---\")\n",
        "validation_dataset = clinc_tokenized['validation']\n",
        "\n",
        "# Evaluate original student model (using the trainer)\n",
        "original_metrics = distilbert_trainer.evaluate(eval_dataset=validation_dataset)\n",
        "print(f\"Original Student Model Accuracy: {original_metrics['eval_accuracy']:.4f}\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluate quantized student model (manual loop for pipeline)\n",
        "clinc_validation_with_text = load_dataset(\"clinc_oos\", \"plus\", split=\"validation\")\n",
        "\n",
        "def evaluate_pipe(pipe, dataset_text, dataset_tokenized):\n",
        "    all_preds = []\n",
        "    all_labels = dataset_tokenized['labels']\n",
        "    for text in tqdm(dataset_text['text']):\n",
        "        pred = pipe(text)[0]['label']\n",
        "        pred_id = label2id[pred]\n",
        "        all_preds.append(pred_id)\n",
        "    return accuracy_score.compute(predictions=all_preds, references=all_labels)\n",
        "\n",
        "quantized_accuracy = evaluate_pipe(quantized_pipe, clinc_validation_with_text, validation_dataset)\n",
        "print(f\"Quantized Student Model Accuracy: {quantized_accuracy['accuracy']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "qvKNCdfIB463",
        "outputId": "8e86c1d5-c29e-4265-b6dd-ed86f9c4af69"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model Size Comparison ---\n",
            "Original Distilled Student Model Size: 255.86 MB\n",
            "Quantized ONNX Student Model Size: 64.39 MB\n",
            "Size reduction of: 74.83%\n",
            "\n",
            "--- Inference Latency Comparison ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Student Model - Average Latency: 67.65 ms/request\n",
            "Quantized Student Model - Average Latency: 13.25 ms/request\n",
            "Achieved a speedup of 5.11x\n",
            "\n",
            "--- Accuracy Comparison ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='130' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 02:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Student Model Accuracy: 0.9248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3100/3100 [00:35<00:00, 87.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Student Model Accuracy: 0.9187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Results & Conclusion\n",
        "\n",
        "This project demonstrates a two-stage optimization pipeline to create a lightweight, high-performance intent classification model.\n",
        "\n",
        "1.  **Knowledge Distillation:** A large, pre-trained BERT-base \"teacher\" model was used to train a smaller DistilBERT \"student\". This transferred the teacher's knowledge, allowing the student to achieve high accuracy quickly while being significantly smaller.\n",
        "2.  **Post-Training Quantization:** The distilled student model was then converted to the ONNX format and quantized from 32-bit floating-point precision to 8-bit integer precision, further reducing its size and dramatically improving inference speed on the CPU.\n",
        "\n",
        "The results below show a massive improvement in efficiency with a negligible drop in performance, making the final model ideal for production deployment.\n",
        "\n",
        "| Model | Parameters | On-Disk Size (MB) | Avg. Latency (CPU) | Validation Accuracy |\n",
        "| :--- | :---: | :---: | :---: | :---: |\n",
        "| **Teacher (BERT-base)** | 110M | 439 MB | 67.65 ms | ~93%* |\n",
        "| **Student (Distilled)** | 67M (-39%) | 256 MB (-42%) | 60.37 ms | **92.48%** |\n",
        "| **Student (Distilled + Quantized)**| 67M (-39%) | **64 MB (-85%)** | **13.25 ms (5.1x speedup)** | **91.87%** (-0.61%) |\n",
        "\n",
        "*Note: Teacher model accuracy is based on the original fine-tuned model's reported performance. The latency was benchmarked on a CPU for a fair comparison.*\n",
        "\n",
        "---\n",
        "\n",
        "### **Final Conclusion**\n",
        "\n",
        "The optimization pipeline was a definitive success. By combining knowledge distillation and post-training quantization, we produced a final model that is:\n",
        "\n",
        "*   **~85% smaller** than the original teacher model (439 MB vs. 64 MB).\n",
        "*   **Over 5 times faster** on a CPU (67.65 ms vs. 13.25 ms).\n",
        "*   All while retaining excellent performance, with a final accuracy of **91.87%**.\n",
        "\n",
        "This process demonstrates a practical, end-to-end MLOps workflow for taking a large, powerful model and making it efficient and cost-effective for real-world deployment."
      ],
      "metadata": {
        "id": "Lz1uMFeAB44W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2_6s-nkB412"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfEV-Z7wB4zc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
